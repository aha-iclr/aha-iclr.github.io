def compute_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):
    # Calculate the distance from the TCP (tool center point) to the handle
    tcp_to_handle_dist = torch.linalg.norm(
        info["handle_link_pos"] - self.agent.tcp.pose.p, axis=1
    )
    reaching_reward = 1 - torch.tanh(5 * tcp_to_handle_dist)

    # Amplify reaching reward to emphasize reaching the handle
    reaching_reward = reaching_reward * 2 # Increased weight
    reward = reaching_reward

    # Check if the handle is being grasped
    is_grasped = self.agent.is_grasping(self.handle_link)

    # Introduce a sharper reward step for grasping
    grasping_reward = torch.where(is_grasped, torch.tensor(3.0), torch.tensor(0.0))
    reward += grasping_reward

    # Calculate how much the drawer is opened
    open_progress = self.handle_link.joint.qpos / self.target_qpos
    opening_reward = torch.clamp(open_progress, 0, 1)

    # Only add opening reward if the handle is grasped
    opening_reward = opening_reward * is_grasped
    reward += opening_reward

    # Check if the link is static
    static_reward = 1 - torch.tanh(
        5 * torch.linalg.norm(self.handle_link.linear_velocity, axis=1)
    )

    # Only add static reward if the handle is grasped and opened enough
    static_reward = static_reward * info["open_enough"] * is_grasped
    reward += static_reward

    # Assign a high reward for successful task completion
    reward[info["success"]] = 5
    return reward

def compute_normalized_dense_reward(
        self, obs: Any, action: torch.Tensor, info: Dict
    ):
    max_reward = 5.0
    return self.compute_dense_reward(obs=obs, action=action, info=info) / max_reward