def compute_dense_reward(self, obs: Any, action: torch.Tensor, info: Dict):
    # Calculate the distance from the TCP (tool center point) to the handle
    tcp_to_handle_dist = torch.linalg.norm(
        info["handle_link_pos"] - self.agent.tcp.pose.p, axis=1
    )
    reaching_reward = 1 - torch.tanh(5 * tcp_to_handle_dist)
    reward = reaching_reward

    # Check if the handle is being grasped
    is_grasped = self.agent.is_grasping(self.handle_link)
    reward += is_grasped

    # Calculate how much the drawer is opened
    open_progress = self.handle_link.joint.qpos / self.target_qpos
    opening_reward = torch.clamp(open_progress, 0, 1)
    reward += opening_reward * is_grasped

    # Check if the link is static
    static_reward = 1 - torch.tanh(
        5 * torch.linalg.norm(self.handle_link.linear_velocity, axis=1)
    )
    reward += static_reward * info["open_enough"]

    # Assign a high reward for successful task completion
    reward[info["success"]] = 5
    return reward

def compute_normalized_dense_reward(
    self, obs: Any, action: torch.Tensor, info: Dict
    ):
    max_reward = 5.0
    return self.compute_dense_reward(obs=obs, action=action, info=info) / max_reward